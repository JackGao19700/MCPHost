import inspect

import copy
import os
import sys

from openai import OpenAI
import re
import requests
import json

import asyncio
from contextlib import AsyncExitStack
from typing import Optional

from mcp import ClientSession,StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.types import CallToolResult

from openai.types.chat import ChatCompletion, ChatCompletionMessage
from openai.types.chat.chat_completion import Choice

from customizedLogger import FileLogger

# 配置日志记录器
myLogger=FileLogger("log/mcphost.log",delay=False)

def restfulAPICall(funcName,funcArgsJson):
    # 调用你的本地 Flask 服务
    flask_response = requests.post(
        "http://localhost:5000/add",
        json=funcArgsJson
    )
    json=flask_response.json() #["result"]
    result = flask_response.json()["result"]
    myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}->> Restful API call result: {result}")

    return str(result)

def toolDeclare(toolname,toolDescription,toolInputSchema):
    toolStr={
        "type": "function",
        "function": {
            "name": toolname,
            "description": toolDescription,
            "parameters": toolInputSchema
        },
    }
    return toolStr

class llmModelWrapper:
    def Chat(self,messages,tools):
        raise NotImplementedError("Subclasses should implement this!")

    def ParseToolCallMessage(self,choice):
        #Choice have 4 field:
        #   1. finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"(deprecated replaced "by tool_calls") ].
        #   2. index: int
        #   3. logprobs: Optional[ChoiceLogprobs] = None  """Log probability information for the choice."""
        #   4. message: ChatCompletionMessage  """A chat completion message generated by the model."""
        raise NotImplementedError("Subclasses should implement this!")

    def getMessageFromChoice(self,choice):
        raise NotImplementedError("Subclasses should implement this!")

class OpenAIModel(llmModelWrapper):
    def __init__(self,envKeyName,apiBaseUrl,modelName):
        self.modelName=modelName
        self.llmClient = OpenAI(api_key=os.environ.get(envKeyName),base_url=apiBaseUrl)

        self.llmClientConfig={
            # "model": self.modelName,
            "max_tokens":1024,
            "temperature":0,
            "stream":False,
        }

    def Chat(self,messages,tools):
        response: ChatCompletion
        response= self.llmClient.chat.completions.create(
            model=self.modelName,
            messages=messages,
            tools=tools,
            **self.llmClientConfig
        )

        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}->> chat response:{response}")
        choice: Choice = response.choices[0]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> chat response.choices[0] {choice}")
        return choice

    def ParseToolCallMessage(self,choice):
        if choice.finish_reason != "tool_calls" and choice.finish_reason != "function_call":
            return []

        message: ChatCompletionMessage = choice.message
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")

        toolsToCall=[]
        if hasattr(choice.message, "tool_calls") and choice.message.tool_calls:
            for tool_call in choice.message.tool_calls:
                # 解析函数名和参数
                func_name = tool_call.function.name
                func_args=json.loads(tool_call.function.arguments)
                tool_call_id=tool_call.id
                myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Function to call:\n\t Tool_call_id:{tool_call_id}\n\tFunction Name:{func_name}\n\targuments:{func_args}")
                toolsToCall.append((func_name,func_args,tool_call_id))
        return toolsToCall

    def getMessageFromChoice(self,choice):
        message: ChatCompletionMessage = choice.message
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")
        return message.role,message.content

    def addMessageFromChoice(self,messages,choice):
        messages.append(choice.message)

class localOllamaModel(llmModelWrapper):
    def __init__(self,modelName,localHost="127.0.0.1",localPort=11434):
        self.modelName=modelName ##本地模型名（如 qwen、mistral 等）
        self.endPointUrl=r"http://"+localHost+":"+str(localPort)+"/v1/chat/completions"

        self.llmClientConfig={
            # "model": self.modelName,
            "max_tokens":1024,
            "temperature":0,
            "stream":False,
        }

    def Chat(self,messages,tools):
        data= copy.deepcopy(self.llmClientConfig)
        data["model"] = self.modelName
        data["messages"]=messages
        data["tools"]=tools

        # 构造 Ollama 的请求（假设使用类似 OpenAI 的格式）
        response = requests.post(
            url=self.endPointUrl,   # Ollama RESTFul endpoint
            json=data
        ).json()

        # 解析 Ollama 的响应
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> chat response:{response}")

        choice = response["choices"][0]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> chat response.choices[0] {choice}")
        return choice

    def ParseToolCallMessage(self,choice):
        if choice["finish_reason"] != "tool_calls" and choice["finish_reason"] != "function_call":
            return []

        message= choice["message"]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")

        toolsToCall=[]
        tool_calls = message.get("tool_calls", [])
        if len(tool_calls)>0:
            for tool_call in tool_calls:
                # 解析函数名和参数
                func_name = tool_call["function"]["name"]
                func_args=json.loads(tool_call["function"]["arguments"])
                tool_call_id=tool_call["id"]
                myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Function to call:\n\t Tool_call_id:{tool_call_id}\n\tFunction Name:{func_name}\n\targuments:{func_args}")

                toolsToCall.append((func_name, func_args, tool_call_id))

        return toolsToCall

    def getMessageFromChoice(self,choice):
        message= choice["message"]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")
        return message["role"],message["content"]

    def addMessageFromChoice(self,messages,choice):
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> message {choice['message']}")
        messages.append(choice["message"])


def serialize_MCPCallToResult(tool_result: CallToolResult) -> str:
    """Convert CallToolResult to a JSON string."""
    serialized_content = []
    for item in tool_result.content:
        if hasattr(item, "text"):  # TextContent
            serialized_content.append({"type": item.type , "text": item.text})
        elif hasattr(item, "data"):  # ImageContent or EmbeddedResource
            serialized_content.append({"type": item.type, "data": item.data,"mimeType":item.mimeType})
        # 其他类型的 content 也可以类似处理

    return json.dumps({
        "content": serialized_content,
        "isError": tool_result.isError,
    })
class MCPHost:
    def __init__(self, llmModel,systemPrompt):
        # Initialize session and client objects
        self.mcpClientSession: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()

        self.llmModel=llmModel
        self.systemPrompt=systemPrompt

    async def connect_to_stdio_server(self,serverParameters:StdioServerParameters):
        self._streams_context = stdio_client(serverParameters)
        streams = await self._streams_context.__aenter__()

        self._session_context = ClientSession(*streams)
        self.mcpClientSession: ClientSession = await self._session_context.__aenter__()

        # Initialize
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Initialized stdio client...")
        await self.mcpClientSession.initialize()

        # List available tools to verify connection
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Listing tools...")
        response = await self.mcpClientSession.list_tools()
        tools = response.tools
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Connected to server with tools: {[tool.name for tool in tools]}")
        self.mcp_tools=[toolDeclare(tool.name,tool.description, tool.inputSchema) for tool in tools]

    async def cleanup(self):
        """Properly clean up the session and streams"""
        try:
            if self._session_context:
                await self._session_context.__aexit__(None, None, None)
            if self._streams_context:
                await self._streams_context.__aexit__(None, None, None)
        except Exception as e:
            myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Cleanup error: {e}")

    async def chatLoop(self):
        """Run an interactive chat loop"""
        print("\nMCP Host Started!")
        print("Type your queries or 'quit' to exit.")
        while True:
            try:
                user_query = input("\nQuery:").strip()
                if user_query.lower() == "quit":
                    break
                response=await self.process_query(user_query)
                print("\n" + response)
            except Exception as e:
                 print(f"Error: {e}")

    async def execute_tool(self, funcName, funcArgs):
        """Execute the tool function with the provided arguments"""
        # MCP tool_call response:
        # 1.content: list[TextContent | ImageContent | EmbeddedResource]
        # 2.isError: bool = False
        result: CallToolResult = await self.mcpClientSession.call_tool(funcName, funcArgs)
        return result

    async def process_query(self, query: str) -> str:
        """Process a query using LLMModel and available tools"""
        messages = []
        systemMessage={
                    "role": "system",
                    "content": self.systemPrompt
                    }
        messages.append(systemMessage)
        userMessage = {
                    "role": "user",
                    "content": query
                }
        messages.append(userMessage)
        choice = self.llmModel.Chat(messages, self.mcp_tools)
        self.llmModel.addMessageFromChoice(messages,choice)

        toolsToCall=self.llmModel.ParseToolCallMessage(choice)
        for funName, funcArgs, tool_call_id in toolsToCall:
            from mcp.types import TextContent
            from typing import Optional
            toolCallResult: Optional[TextContent] = None
            toolCallResult=await self.execute_tool(funName, funcArgs)

            content=toolCallResult.content
            isError=toolCallResult.isError

            # result=restfulAPICall(func_name,func_args)
            myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> tool_call\n\tfun name:{funName}\n\t isError:{isError}\n\treturn content:{content}")
            #myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> tool_call content[0].\n   type:{type(content[0])}\n   content:{content[0]}")

            toolMessage = {"role": "tool",
                           "tool_call_id": tool_call_id,
                           "content": serialize_MCPCallToResult(toolCallResult)}
            messages.append(toolMessage)

        choice = self.llmModel.Chat(messages,tools=None)
        role,content=self.llmModel.getMessageFromChoice(choice)
        return f"Assistant>\t {content}"

async def main(llmModel:llmModelWrapper,systemPrompt:str,serverParameters:StdioServerParameters):
    # if len(sys.argv)<2:
    #     print("Usage: uv run client.py <URL of SSE MCP server (i.e. http://localhost:8080/sse)>")
    #     sys.exit(1)

    myMCPHost=MCPHost(llmModel,systemPrompt)
    try:
        await myMCPHost.connect_to_stdio_server(serverParameters)
        await myMCPHost.chatLoop()
    except Exception as e:
        print(f"An error occurred: {e}")
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> running error: {e}")
    finally:
        await myMCPHost.cleanup()

if __name__ == "__main__":
    api_key_name = "DEEPSEEK_API_KEY"
    api_base_url="https://api.deepseek.com"
    llmModel=OpenAIModel(api_key_name,api_base_url,"deepseek-chat")
    #llmModel=localOllamaModel("qwen2.5:latest")
    query="请调用'J:/'下有哪些文件?"

    systemPrompt="你是一个智能助手，你的名字叫Jack.请调用工具,然后回答用户问题"
    serverParameters=StdioServerParameters(
        command="npx",  # Executable
        args=[
                "-y",
                "@modelcontextprotocol/server-filesystem",
                "J:\\",
                "J:\\mcpserverDemo\\mcpClient"
            ],
        env={"MCP_SERVER": "weather"}
    )

    # 使用 asyncio.run 来运行异步函数
    asyncio.run(main(llmModel,systemPrompt,serverParameters))

