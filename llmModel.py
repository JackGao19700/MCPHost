import inspect
import json
import copy
import requests
import os

from openai import OpenAI
from openai.types.chat import ChatCompletion, ChatCompletionMessage
from openai.types.chat.chat_completion import Choice

from helperFun import myLogger
class llmModelWrapper:
    def Chat(self,messages,tools):
        raise NotImplementedError("Subclasses should implement this!")

    def ParseToolCallMessage(self,choice):
        #Choice have 4 field:
        #   1. finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"(deprecated replaced "by tool_calls") ].
        #   2. index: int
        #   3. logprobs: Optional[ChoiceLogprobs] = None  """Log probability information for the choice."""
        #   4. message: ChatCompletionMessage  """A chat completion message generated by the model."""
        raise NotImplementedError("Subclasses should implement this!")

    def getMessageFromChoice(self,choice):
        raise NotImplementedError("Subclasses should implement this!")

class OpenAIModel(llmModelWrapper):
    def __init__(self,envKeyName,apiBaseUrl,modelName):
        self.modelName=modelName
        self.llmClient = OpenAI(api_key=os.environ.get(envKeyName),base_url=apiBaseUrl)

        self.llmClientConfig={
            # "model": self.modelName,
            "max_tokens":1024,
            "temperature":0,
            "stream":False,
        }

    def Chat(self,messages,tools):
        response: ChatCompletion
        response= self.llmClient.chat.completions.create(
            model=self.modelName,
            messages=messages,
            tools=tools,
            **self.llmClientConfig
        )

        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}->> chat response:{response}")
        choice: Choice = response.choices[0]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> chat response.choices[0] {choice}")
        return choice

    def ParseToolCallMessage(self,choice):
        if choice.finish_reason != "tool_calls" and choice.finish_reason != "function_call":
            return []

        message: ChatCompletionMessage = choice.message
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")

        toolsToCall=[]
        if hasattr(choice.message, "tool_calls") and choice.message.tool_calls:
            for tool_call in choice.message.tool_calls:
                # 解析函数名和参数
                func_name = tool_call.function.name
                func_args=json.loads(tool_call.function.arguments)
                tool_call_id=tool_call.id
                myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Function to call:\n\t Tool_call_id:{tool_call_id}\n\tFunction Name:{func_name}\n\targuments:{func_args}")
                toolsToCall.append((func_name,func_args,tool_call_id))
        return toolsToCall

    def getMessageFromChoice(self,choice):
        message: ChatCompletionMessage = choice.message
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")
        return message.role,message.content

    def addMessageFromChoice(self,messages,choice):
        messages.append(choice.message)

class localOllamaModel(llmModelWrapper):
    def __init__(self,modelName,localHost="127.0.0.1",localPort=11434):
        self.modelName=modelName ##本地模型名（如 qwen、mistral 等）
        self.endPointUrl=r"http://"+localHost+":"+str(localPort)+"/v1/chat/completions"

        self.llmClientConfig={
            # "model": self.modelName,
            "max_tokens":1024,
            "temperature":0,
            "stream":False,
        }

    def Chat(self,messages,tools):
        data= copy.deepcopy(self.llmClientConfig)
        data["model"] = self.modelName
        data["messages"]=messages
        data["tools"]=tools

        # 构造 Ollama 的请求（假设使用类似 OpenAI 的格式）
        response = requests.post(
            url=self.endPointUrl,   # Ollama RESTFul endpoint
            json=data
        ).json()

        # 解析 Ollama 的响应
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> chat response:{response}")

        choice = response["choices"][0]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> chat response.choices[0] {choice}")
        return choice

    def ParseToolCallMessage(self,choice):
        if choice["finish_reason"] != "tool_calls" and choice["finish_reason"] != "function_call":
            return []

        message= choice["message"]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")

        toolsToCall=[]
        tool_calls = message.get("tool_calls", [])
        if len(tool_calls)>0:
            for tool_call in tool_calls:
                # 解析函数名和参数
                func_name = tool_call["function"]["name"]
                func_args=json.loads(tool_call["function"]["arguments"])
                tool_call_id=tool_call["id"]
                myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> Function to call:\n\t Tool_call_id:{tool_call_id}\n\tFunction Name:{func_name}\n\targuments:{func_args}")

                toolsToCall.append((func_name, func_args, tool_call_id))

        return toolsToCall

    def getMessageFromChoice(self,choice):
        message= choice["message"]
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> response message {message}")
        return message["role"],message["content"]

    def addMessageFromChoice(self,messages,choice):
        myLogger(f"<Fun:{inspect.currentframe().f_code.co_name}> message {choice['message']}")
        messages.append(choice["message"])

